{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# FLIP (02): Modern Data Science\n",
    "**(Module 02:Probablistic Graphical Models )**\n",
    "\n",
    "---\n",
    "- Materials in this module include resources collected from various open-source online repositories.\n",
    "- You are free to use, but NOT allowed to change or distribute this package.\n",
    "\n",
    "Prepared by and for \n",
    "**Student Members** |\n",
    "2006-2018 [TULIP Lab](http://www.tulip.org.au), Australia\n",
    "\n",
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Contents\n",
    "1. [Hamiltonian Monte Carlo](cell_haniltonian)\n",
    "2. [No-U-Turn Sample](cell_No)\n",
    "\n",
    "## Hamiltonian Monte Carlo\n",
    "Hamiltonian Monte Carlo (HMC) is a Markov Chain Monte Carlo (MCMC) that proposes future states in Markov Chain using\n",
    "Hamiltonian Dynamics. Before understanding the HMC algorithm lets first understand Hamiltonian Dynamics.\n",
    "\n",
    "### Hamiltonian Dynamics\n",
    "Hamiltonian dynamics are used to describe how objects move throughout a\n",
    "system. Hamiltonian dynamics is defined in terms of object location $x$\n",
    "and its momentum $p$ (equivalent to object's mass times velocity) at some\n",
    "time $t$. For each location of object there is an associated potential\n",
    "energy $U(x)$ and with momentum there is associated \n",
    "kinetic energy $K(p)$. The total energy of system is constant and is called as Hamiltonian \n",
    "$H(x, p)$, defined as the sum of potential energy and kinetic energy:\n",
    "\n",
    "$$ H(x, p) = U(x) + K(p) $$\n",
    "\n",
    "The partial derivatives of the Hamiltonian determines how position $x$ and\n",
    "momentum $p$ change over time $t$, according to Hamiltonian's equations:\n",
    "\n",
    "$$ \\frac{dx_i}{dt} = \\frac{\\partial H}{\\partial p_i} = \\frac{\\partial K(p)}{\\partial p_i}$$\n",
    "\n",
    "$$ \\frac{dp_i}{dt} = -\\frac{\\partial H}{\\partial x_i} = -\\frac{\\partial U(x)}{\\partial x_i}$$\n",
    "\n",
    "The above equations operates on a *d-dimensional position vector $x$* and\n",
    "a *d-dimensional momentum vector $p$*, for $i = 1, 2, \\cdots, d$.\n",
    "\n",
    "Thus, if we can evaluate $\\frac{\\partial U(x)}{\\partial x_i}$ and \n",
    "$\\frac{\\partial K(p)}{\\partial p_i}$ and have a set of initial conditions i.e\n",
    "an initial position and initial momentum at time $t_0$, then we can predict\n",
    "the location and momentum of object at any future time $t = t_0 + T$ by\n",
    "simulating dynamics for a time duration $T$.\n",
    "### Discretizing Hamiltonian's Equations\n",
    "\n",
    "The Hamiltonian's equations describes an object's motion in regard to time,\n",
    "which is a continuous variable. For simulating dynamics on a computer,\n",
    "Hamiltonian's equations must be numerically approximated by discretizing time.\n",
    "This is done by splitting the time interval $T$ into small intervals of size\n",
    "$\\epsilon$.\n",
    "\n",
    "#### Euler's Method\n",
    "For Hamiltonian's equations, this method performs the following steps, for\n",
    "each component of position and momentum (indexed by $i=1, ...,d$)\n",
    "\n",
    "$$ p_i(t + \\epsilon) = p_i(t) + \\epsilon \\frac{dp_i}{dt}(t) = p_i(t) - \\epsilon \\frac{\\partial U}{\\partial x_i(t)} $$\n",
    "\n",
    "$$ x_i(t + \\epsilon) = x_i(t) + \\epsilon \\frac{dx_i}{dt} = x_i(t) + \\epsilon \\frac{\\partial K}{\\partial p_i(t)} $$\n",
    "\n",
    "Even better results can be obtained if we use updated value of momentum\n",
    "in later equation\n",
    "\n",
    "$$ x_i(t + \\epsilon) = x_i(t) + \\epsilon \\frac{\\partial K}{\\partial p_i(t + \\epsilon)} $$\n",
    "\n",
    "This method is called as **Modified Euler's method**.\n",
    "\n",
    "#### Leapfrog Method\n",
    "Unlike Euler's method where we take full steps for updating position and\n",
    "momentum in leapfrog method we take half steps to update momentum value.\n",
    "\n",
    "$$ p_i(t + \\epsilon / 2) = p_i(t) - (\\epsilon / 2) \\frac{\\partial U}{\\partial x_i(t)} $$\n",
    "\n",
    "$$x_i(t + \\epsilon) = x_i(t) + \\epsilon \\frac{\\partial K}{\\partial p_i(t + \\epsilon /2)}  $$\n",
    "\n",
    "$$ p_i(t + \\epsilon) = p_i(t) - (\\epsilon / 2) \\frac{\\partial U}{\\partial x_i(t + \\epsilon)} $$\n",
    "\n",
    "Leapfrog method yields even better result than Modified Euler Method.\n",
    "\n",
    "#### Example: Simulating Hamiltonian dynamics of a simple pendulum\n",
    "\n",
    "Imagine a bob of mass $m = 1$ attached to a string of length $l=1.5$\n",
    "whose one end is fixed at point $(x=0, y=0)$.\n",
    "The equilibrium position of the pendulum is at $x = 0$. Now keeping string\n",
    "stretched we move it some distance horizontally say $x_0$. The corresponding\n",
    "change in potential energy is given by\n",
    "\n",
    "$ U(h) = mg\\Delta h $, where $\\Delta h$ is change in height and $g$ is gravity of earth.\n",
    "\n",
    "Using simple trigonometry one can derive relationship between $x$ and \n",
    "$\\Delta h$.\n",
    "\n",
    "$$ U(x) = mgl(1 - cos(sin^{-1}(x/l)))$$\n",
    "\n",
    "Kinetic energy of bob can be written in terms of momentum as\n",
    "\n",
    "$$ K(v) = \\frac{mv^2}{2} = \\frac{(mv)^2}{2m} = \\frac{p^2}{2m} = K(p)$$\n",
    "\n",
    "Further, partial derivatives of potential and kinetic energy can be written as:\n",
    "\n",
    "$$ \\frac{\\partial U}{\\partial x} = \\frac{mglx}{\\sqrt{l^2 - x^2}}$$\n",
    "\n",
    "and\n",
    "\n",
    "$$ \\frac{\\partial K}{\\partial p} = \\frac{p}{m} $$\n",
    "\n",
    "Here is a animation that uses these equations to simulate the dynamics of simple pendulum\n",
    "<img src=\"../images/simple_pendulum.gif\" alt=\"Drawing\" style=\"width: 700px;\"/>\n",
    "The sub-plot in the right upper half of the output demonstrates the energies. The\n",
    "red portion of first bar plot represents potential energy and black represents\n",
    "kinetic energy. The second bar plot represents the Hamiltonian. The lower right sub-plot shows the phase space\n",
    "showing how momentum and position are varying. We can see that phase space\n",
    "maps out an ellipse without deviating from its path. In case of Euler\n",
    "method the particle doesn't fully trace a ellipse instead diverges slowly \n",
    "towards infinity. One can clearly see that value of position and momentum are not completely\n",
    "random, but takes a deterministic circular kind of trajectory.\n",
    "If we use Leapfrog method to propose future states than we can avoid\n",
    "random-walk behavior which we saw in Metropolis-Hastings algorithm. This is the main reason for good performance\n",
    "of HMC algorithm.\n",
    "\n",
    "### Hamiltonian and Probability: Canonical Distributions\n",
    "\n",
    "Now having a bit of understanding what is Hamiltonian and how we can simulate\n",
    "Hamiltonian dynamics, we now need to understand how we can use these\n",
    "Hamiltonian dynamics for MCMC. We need to develop some relation between\n",
    "probability distribution and Hamiltonian so that we can use Hamiltonian\n",
    "dynamics to explore the distribution. To relate $H(x, p)$ to target\n",
    "distribution $P(x)$ we use a concept from statistical mechanics known as\n",
    "the canonical distribution. For any energy function $E(q)$, defined over a set of variables $q$, we\n",
    "can find corresponding $P(q)$\n",
    "\n",
    "$$ P(q) = \\frac{1}{Z} exp \\left( \\frac{-E(q)}{T} \\right) $$\n",
    "\n",
    ", where $Z$ is normalizing constant called Partition function  and $T$ is\n",
    "temperature of system. For our use case we will consider $T=1$.\n",
    "\n",
    "Since, the Hamiltonian is an energy function for the joint state of \"position\",\n",
    "$x$ and \"momentum\", $p$, so we can define a joint distribution for them\n",
    "as follows:\n",
    "\n",
    "$$ P(x, p) = \\frac{e^{-H(x, p)}}{Z} $$\n",
    "\n",
    "Since $H(x, p) = U(x) + K(p)$, we can write above equation as\n",
    "\n",
    "$$P(x, p) = \\frac{e^{-U(x)-K(p)}}{z}$$\n",
    "\n",
    "$$P(x, p) = \\frac{e^{-U(x)}e^{-K(p)}}{Z}$$\n",
    "\n",
    "Furthermore we can associate probability distribution with each of the\n",
    "potential and kinetic energy ($P(x)$ with potential energy and $P(p)$,\n",
    "with kinetic energy). Thus, we can write above equation as:\n",
    "\n",
    "$$P(x, p) = \\frac{P(x)P(p)}{Z'} $$\n",
    "\n",
    ",where $Z'$ is new normalizing constant. Since joint distribution factorizes\n",
    "over $x$ and $p$, we can conclude that $P(x)$ and $P(p)$ are independent.\n",
    "Because of this independence we can choose any distribution\n",
    "from which we want to sample the momentum variable. A common choice is to use\n",
    "a zero mean and unit variance Normal distribution $N(0, I)$ \n",
    "The target distribution of interest $P(x)$ from which we actually want to\n",
    "sample from is associated with potential energy.\n",
    "\n",
    "$$U(x) = - log (P(x))$$\n",
    "\n",
    "Thus, if we can calculate $\\frac{\\partial log(P(x))}{\\partial x_i}$, then\n",
    "we are in business and we can use Hamiltonian dynamics to generate samples.\n",
    "\n",
    "### Hamiltonian Monte Carlo Algorithm\n",
    "Given initial state $x_0$, stepsize $\\epsilon$, number of steps $L$, \n",
    "log density function $U$, number of samples to be drawn $M$, we can write HMC algorithm as:\n",
    "\n",
    "- set $m = 0 $\n",
    "- repeat until $m = M$\n",
    "\n",
    "    1. set  $m \\leftarrow m + 1$\n",
    "\n",
    "    2. Sample new initial momentum $p_0$ ~ $N(0, I)$\n",
    "    \n",
    "    3. Set $x_m \\leftarrow x_{m-1}, x' \\leftarrow x_{m-1}, p' \\leftarrow p_0$\n",
    "\n",
    "    4. repeat for $L$ steps\n",
    "\n",
    "        - Set $x', p' \\leftarrow Leapfrog(x', p', \\epsilon)$\n",
    "\n",
    "    5. Calculate acceptance probability $\\alpha = min \\left(1, \\frac{exp( U(x') - (p'.p')/2 )}{exp( U(x_{m-1}) - (p_0.p_0)/2 )} \\right)$\n",
    "\n",
    "    6. Draw a random number u ~ Uniform(0, 1)\n",
    " \n",
    "    7. if $u \\leq \\alpha$ then  $x_m \\leftarrow x', p_m \\leftarrow -p'$\n",
    "\n",
    "$Leapfrog$ is a function that runs a single iteration of Leapfrog method.\n",
    "\n",
    "In practice sometimes instead of explicitly giving number of steps $L$, \n",
    "we use **trajectory length** which is product of number of steps $L$,\n",
    "and stepsize $\\epsilon$.\n",
    "\n",
    "### Hamiltonian Monte Carlo in pgmpy\n",
    "In pgmpy one can use Hamiltonian Monte Carlo algorithm by importing HamiltonianMC from pgmpy.inference.continuous "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pgmpy.inference.continuous import HamiltonianMC as HMC"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Lets use the HamiltonianMC implementation and draw some samples from a multivariate disrtibution $ P(x) = N(\\mu, \\Sigma)$, where\n",
    "$$\n",
    "\\mu = [0, 0], \\qquad\n",
    "\\Sigma = \\left[\n",
    "    \\begin{array}{cc}\n",
    "    1 & 0.97 \\\\\n",
    "    0.97 & 1\n",
    "    \\end{array}\n",
    "    \\right]\n",
    "$$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%matplotlib inline\n",
    "from pgmpy.factors import JointGaussianDistribution\n",
    "from pgmpy.inference.continuous import LeapFrog, GradLogPDFGaussian\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "np.random.seed(77777)\n",
    "# Defining a multivariate distribution model\n",
    "mean = np.array([0, 0])\n",
    "covariance = np.array([[1, 0.97], [0.97, 1]])\n",
    "model = JointGaussianDistribution(['x', 'y'], mean, covariance)\n",
    "\n",
    "# Creating a HMC sampling instance\n",
    "sampler = HMC(model=model, grad_log_pdf=GradLogPDFGaussian, simulate_dynamics=LeapFrog)\n",
    "# Drawing samples\n",
    "samples = sampler.sample(initial_pos=np.array([7, 0]), num_samples = 1000,\n",
    "                         trajectory_length=10, stepsize=0.25)\n",
    "plt.figure(figsize=(7, 7)); plt.hold(True)\n",
    "plt.scatter(samples['x'], samples['y'], label='HMC samples', color='k')\n",
    "plt.plot(samples['x'][0:100], samples['y'][0:100], 'r-', label='First 100 samples')\n",
    "plt.legend(); plt.hold(False)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "One, can change the values of parameters `stepsize` and `trajectory_length` and see the convergence towards target distribution. For example set the value of `stepsize = 0.5` and let rest parameters have the same value and see the output. With this you might get a feel that performance of HMC critically depends upon choice of these parameters.\n",
    "\n",
    "The `stepsize` parameter for HamiltonianMC implementation is optional, but should be use only as starting point value.\n",
    "One should hand-tune the model using this stepsize value for good performance.\n",
    "\n",
    "### Hamiltonian Monte Carlo with dual averaging\n",
    "In pgmpy we have implemented an another variant of HMC in which we adapt the stepsize during the course of sampling thus\n",
    "completely eliminates the need of specifying `stepsize` (but still requires\n",
    "`trajectory_length` to be specified by user). This variant of HMC is known as Hamiltonian Monte Carlo with dual averaging (HamiltonianMCda in pgmpy). One can also use Modified Euler to simulate dynamics instead of leapfrog, or even can plug-in one's own implementation for simulating dynamics using base class for it."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pgmpy.inference.continuous import HamiltonianMCda as HMCda, ModifiedEuler\n",
    "# Using modified euler instead of Leapfrog for simulating dynamics\n",
    "sampler_da = HMCda(model, GradLogPDFGaussian, simulate_dynamics=ModifiedEuler)\n",
    "# num_adapt is number of iteration to run adaptation of stepsize\n",
    "samples = sampler_da.sample(initial_pos=np.array([7, 0]), num_adapt=10, num_samples=10, trajectory_length=10)\n",
    "print(samples)\n",
    "print(\"\\nAcceptance rate:\",sampler_da.acceptance_rate)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The values returned by HamiltonianMC and HamiltonianMCda depends upon the installation available in the working environment. In working env has a installation of pandas, it returns a pandas.DataFrame object otherwise it returns a\n",
    "numpy.recarry (numpy recorded arrays).\n",
    "\n",
    "Lets now use base class for simulating hamiltonian dynamics and write our own Modified Euler method"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pgmpy.inference.continuous import BaseSimulateHamiltonianDynamics\n",
    "\n",
    "class ModifiedEulerMethod(BaseSimulateHamiltonianDynamics):\n",
    "    def __init__(self, model, position, momentum, stepsize, grad_log_pdf, grad_log_position=None):\n",
    "\n",
    "        BaseSimulateHamiltonianDynamics.__init__(self, model, position, momentum,\n",
    "                                                 stepsize, grad_log_pdf, grad_log_position)\n",
    "\n",
    "        self.new_position, self.new_momentum, self.new_grad_logp = self._get_proposed_values()\n",
    "\n",
    "    def _get_proposed_values(self):\n",
    "        new_momentum = self.momentum + self.stepsize * self.grad_log_position\n",
    "        new_position = self.position + self.stepsize * new_momentum\n",
    "\n",
    "        grad_log, _ = self.grad_log_pdf(new_position, self.model).get_gradient_log_pdf()\n",
    "\n",
    "        return new_position, new_momentum, grad_log\n",
    "\n",
    "hmc_sampler = HMC(model, GradLogPDFGaussian, simulate_dynamics=ModifiedEulerMethod)\n",
    "samples = hmc_sampler.sample(initial_pos=np.array([0, 0]), num_samples=10, trajectory_length=10, stepsize=0.2)\n",
    "print(samples)\n",
    "print(\"Total accepted proposal:\", hmc_sampler.accepted_proposals)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## No-U-Turn Sampler\n",
    "Both (HMC and HMCda) of these algorithms requires some hand-tuning from user,\n",
    "which can be time consuming especially for high dimensional complex model.\n",
    "No-U-Turn Sampler(NUTS) is an extension of HMC that eliminates the need to specify the\n",
    "trajectory length but requires user to specify stepsize.\n",
    "\n",
    "NUTS, removes the need of parameter number of steps by considering a metric\n",
    "to evaluate whether we have ran Leapfrog algorithm for long enough, that\n",
    "is when running the simulation for more steps would no longer increase\n",
    "the distance between the proposal value of $x$ and initial value of\n",
    "$x$\n",
    "\n",
    "At high level, NUTS uses the leapfrog method to trace out a path forward\n",
    "and backward in fictitious time, first running forwards or backwards\n",
    "1 step, the forwards and backwards 2 steps, then forwards or backwards\n",
    "4 steps etc. This doubling process builds a balanced binary tree whose\n",
    "leaf nodes correspond to position-momentum states. The doubling process is\n",
    "halted when the sub-trajectory from the leftmost to the rightmost nodes of\n",
    "any balanced subtree of the overall binary tree starts to double back on\n",
    "itself (i.e., the  fictional particle starts to make a \"U-Turn\"). At\n",
    "this point NUTS stops the simulation and samples from among the set of\n",
    "points computed during  the  simulation, taking are to preserve detailed\n",
    "balance.\n",
    "\n",
    "Lets use NUTS and draw some samples."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pgmpy.inference.continuous import NoUTurnSampler as NUTS\n",
    "mean = np.array([1, 2, 3])\n",
    "covariance = np.array([[2, 0.4, 0.5], [0.4, 3, 0.6], [0.5, 0.6, 4]])\n",
    "model = JointGaussianDistribution(['x', 'y', 'z'], mean, covariance)\n",
    "\n",
    "# Creating sampling instance of NUTS\n",
    "NUTS_sampler = NUTS(model=model, grad_log_pdf=GradLogPDFGaussian)\n",
    "samples = NUTS_sampler.sample(initial_pos=[10, 10, 0], num_samples=2000, stepsize=0.25)\n",
    "\n",
    "from mpl_toolkits.mplot3d import Axes3D\n",
    "fig = plt.figure(figsize=(9, 9))\n",
    "ax = Axes3D(fig)\n",
    "plt.hold(True)\n",
    "ax.scatter(samples['x'], samples['y'], samples['z'], label='NUTS Samples')\n",
    "ax.plot(samples['x'][:50], samples['y'][:50], samples['z'][:50], 'r-', label='Warm-up period')\n",
    "plt.legend()\n",
    "plt.title(\"Scatter plot of samples\")\n",
    "plt.hold(False)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The **Warm-up period** a.k.a **Burn-in period** of Markov chain , is the amount of time it takes for the markov chain\n",
    "to reach the target stationary distribution. The samples generated during this period of Markov chain are usually thrown away because they don't show the characteristics of distribution from which they are sampled.\n",
    "\n",
    "Since it is difficult to visualize more than 3-Dimensions we generally use a trace-plot of Markov chain to determine this\n",
    "warm-up period."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.figure(figsize=(9, 9))\n",
    "plt.hold(True)\n",
    "plt.plot(samples['x'],'b-', label='x')\n",
    "plt.plot(samples['y'], 'g-', label='y')\n",
    "plt.plot(samples['z'], 'c-', label='z')\n",
    "plt.plot([50, 50], [-5, 9], 'k-', label='Warm-up period', linewidth=4)\n",
    "plt.legend()\n",
    "plt.title(\"Trace plot of Markov Chain\")\n",
    "plt.hold(False)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### No-U-Turn Sampler with dual averaging\n",
    "Like HMCda in No-U-Turn sampler with dual averaging (NUTSda) we adapt the stepsize during the course of sampling thus completely eliminates the need of specifying `stepsize`. Thus we can use NUTSda without any hand tuning at all."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pgmpy.inference.continuous import NoUTurnSamplerDA as NUTSda\n",
    "NUTSda_sampler = NUTSda(model=model, grad_log_pdf=GradLogPDFGaussian)\n",
    "samples = NUTSda_sampler.sample(initial_pos=[0.457420, 0.500307, 0.211056], num_adapt=10, num_samples=10)\n",
    "print(samples)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Apart from the `sample` method all the four algorithms (HMC, HMCda, NUTS, NUTSda) provides another method to sample from model named `generate_sample` method. `generate_sample` method returns a generator type object whose each iteration yields a sample. The sample returned is a simple numpy.array object. The arguments for `generate_sample` method and `sample` method for all algorithms are same"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "generate_samples = NUTSda_sampler.generate_sample(initial_pos=[0.4574, 0.503, 0.211], num_adapt=10, num_samples=10)\n",
    "samples = np.array([sample for sample in generate_samples])\n",
    "print(samples)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Support for coustom Models\n",
    "One can also use HMC, HMCda, NUTS and NUTSda to sample from a user defined model, all one has to do is create class for finding log of probability density function and gradient log of probability density function using base class provided by pgmpy, and give this class as a parameter to `grad_log_pdf` argument in all of these algorithms (HMC[da], NUTS[da]).\n",
    "\n",
    "In this example we will define our own logisitcs distribution and use NUTSda to sample from it.\n",
    "\n",
    "The probability density of logistic distribution is given by:\n",
    "\n",
    "$$ P(x; \\mu, s) = \\frac{e^{-\\frac{x- \\mu}{s}}}{s(1 + e^{-\\frac{x - \\mu}{s}})^2} $$\n",
    "\n",
    "Thus the log of this probability density function (potential energy function) can be written as:\n",
    "\n",
    "$$ log(P(x; \\mu, s)) = -\\frac{x - \\mu}{s} - log(s) - 2 log(1 + e^{-\\frac{x - \\mu}{s}}) $$\n",
    "\n",
    "And the gradient of potential energy :\n",
    "\n",
    "$$ \\frac{\\partial log(P(x; \\mu, s))}{\\partial x} = - \\frac{1}{s} + \\frac{2e^{-\\frac{x - \\mu}{s}}}{s(1 + e^{-\\frac{x - \\mu}{s}})}$$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Importing th base class structure for log and gradient log of probability density function\n",
    "from pgmpy.inference.continuous import BaseGradLogPDF\n",
    "\n",
    "# Base class for user defined continuous factor\n",
    "from pgmpy.factors import ContinuousFactor\n",
    "\n",
    "\n",
    "# Defining pdf of a Logistic distribution with mu = 5, s = 2\n",
    "def logistic_pdf(x):\n",
    "    power = - (x - 5.0) / 2.0\n",
    "    return np.exp(power) / (2 * (1 + np.exp(power))**2)\n",
    "\n",
    "# Calculating log of logistic pdf\n",
    "def log_logistic(x):\n",
    "    power = - (x - 5.0) / 2.0\n",
    "    return power - np.log(2.0) - 2 * np.log(1 + np.exp(power))\n",
    "\n",
    "# Calculating gradient log of logistic pdf\n",
    "def grad_log_logistic(x):\n",
    "    power = - (x - 5.0) / 2.0\n",
    "    return - 0.5 - (2 / (1 + np.exp(power))) * np.exp(power) * (-0.5)\n",
    "\n",
    "# Creating a logistic model\n",
    "logistic_model = ContinuousFactor(['x'], logistic_pdf)\n",
    "\n",
    "# Creating a class using base class for gradient log and log probability density function\n",
    "class GradLogLogistic(BaseGradLogPDF):\n",
    "\n",
    "    def __init__(self, variable_assignments, model):\n",
    "        BaseGradLogPDF.__init__(self, variable_assignments, model)\n",
    "        self.grad_log, self.log_pdf = self._get_gradient_log_pdf()\n",
    "\n",
    "    def _get_gradient_log_pdf(self):\n",
    "        return (grad_log_logistic(self.variable_assignments),\n",
    "                log_logistic(self.variable_assignments))\n",
    "\n",
    "# Generating samples using NUTS\n",
    "sampler = NUTSda(model=logistic_model, grad_log_pdf=GradLogLogistic)\n",
    "samples = sampler.sample(initial_pos=np.array([0.0]), num_adapt=10000,\n",
    "                         num_samples=10000)\n",
    "\n",
    "x = np.linspace(-30, 30, 10000)\n",
    "y = [logistic_pdf(i) for i in x]\n",
    "plt.figure(figsize=(8, 8))\n",
    "plt.hold(1)\n",
    "plt.plot(x, y, label='real logistic pdf')\n",
    "plt.hist(samples.values, normed=True, histtype='step', bins=200, label='Samples NUTSda')\n",
    "plt.legend()\n",
    "plt.hold(0)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "anaconda-cloud": {},
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
